<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matthew Schlegel</title>
    <link>https://mkschleg.github.io/</link>
    <description>Recent content on Matthew Schlegel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mkschleg.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predictive Processing</title>
      <link>https://mkschleg.github.io/braindump/predictive_processing/</link>
      <pubDate>Thu, 21 May 2020 10:19:58 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/predictive_processing/</guid>
      <description>Backlinks 3 Backlinks Free-Energy Principle Predictive Processing,
Prediction, cognition and the brain Predictive Processing, Brain
Whatever next? Predictive brains, situated agents, and the future of cognitive science Predictive Processing, Perception</description>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://mkschleg.github.io/braindump/reinforcement_learning/</link>
      <pubDate>Thu, 21 May 2020 10:19:58 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/reinforcement_learning/</guid>
      <description>Backlinks 5 Backlinks Rich Sutton People, Reinforcement Learning
Temporal Difference Learning Reinforcement Learning
Value Function Reinforcement Learning
Learning to predict by the methods of temporal differences. 1988, Sutton, R.S. Impact*: This paper has had a long lasting impact. Primarily, in establishing temporal difference learning and in the process establishing the Reinforcement Learning field.
Reinforcement Learning</description>
    </item>
    
    <item>
      <title>Thinking</title>
      <link>https://mkschleg.github.io/braindump/thinking/</link>
      <pubDate>Thu, 21 May 2020 10:19:58 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/thinking/</guid>
      <description>Backlinks 1 Backlinks How to Take Smart Notes with Org-mode Thinking, Note-Taking</description>
    </item>
    
    <item>
      <title>Neuroscience</title>
      <link>https://mkschleg.github.io/braindump/neuroscience/</link>
      <pubDate>Thu, 21 May 2020 10:19:57 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/neuroscience/</guid>
      <description>Backlinks 2 Backlinks Afferent Codes Brain, Neuroscience
Visual System Brain, Neuroscience, Anatomy</description>
    </item>
    
    <item>
      <title>Note-Taking</title>
      <link>https://mkschleg.github.io/braindump/note_taking/</link>
      <pubDate>Thu, 21 May 2020 10:19:57 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/note_taking/</guid>
      <description>Backlinks 2 Backlinks Zettelkasten method Note-Taking
How to Take Smart Notes with Org-mode Thinking, Note-Taking</description>
    </item>
    
    <item>
      <title>Optimizers</title>
      <link>https://mkschleg.github.io/braindump/optimizers/</link>
      <pubDate>Thu, 21 May 2020 10:19:57 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/optimizers/</guid>
      <description>Backlinks 1 Backlinks ADAM Optimizers, Neural Network</description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://mkschleg.github.io/braindump/people/</link>
      <pubDate>Thu, 21 May 2020 10:19:57 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/people/</guid>
      <description>Backlinks 2 Backlinks Hermann von Helmholtz People
Rich Sutton People, Reinforcement Learning</description>
    </item>
    
    <item>
      <title>Perception</title>
      <link>https://mkschleg.github.io/braindump/perception/</link>
      <pubDate>Thu, 21 May 2020 10:19:57 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/perception/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Philosophy</title>
      <link>https://mkschleg.github.io/braindump/philosophy/</link>
      <pubDate>Thu, 21 May 2020 10:19:57 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/philosophy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Linear Algebra</title>
      <link>https://mkschleg.github.io/braindump/linear_algebra/</link>
      <pubDate>Thu, 21 May 2020 10:19:56 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/linear_algebra/</guid>
      <description>tags Math  Backlinks 5 Backlinks Linear Algebra and its Applications Linear Algebra, Math
Determinant Linear Algebra, Math
Gaussian Elimination Linear Algebra, Math
Hadamard product Linear Algebra
Matrix Linear Algebra, Math</description>
    </item>
    
    <item>
      <title>Linear Algebra and its Applications</title>
      <link>https://mkschleg.github.io/braindump/linear_algebra_and_its_applications/</link>
      <pubDate>Thu, 21 May 2020 10:19:56 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/linear_algebra_and_its_applications/</guid>
      <description>tags Linear Algebra, Math  Author: Gilbert Strang Available at: Biblio
It is a bit thick and maybe too detailed in the calculations of the certain properties. Really, all I need is understanding the properties of objects, rather than worrying about the details of computation.
I&amp;rsquo;ve only read bits and pieces of the book and am moving on to other books.</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://mkschleg.github.io/braindump/machine_learning/</link>
      <pubDate>Thu, 21 May 2020 10:19:56 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/machine_learning/</guid>
      <description>Backlinks 4 Backlinks Neural Network Machine Learning
LSTM Recurrent Neural Network, Neural Network, Machine Learning
Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies Recurrent Neural Network, Machine Learning
On Multiplicative Integration with Recurrent Neural Networks Recurrent Neural Network, Neural Network, Machine Learning</description>
    </item>
    
    <item>
      <title>Math</title>
      <link>https://mkschleg.github.io/braindump/math/</link>
      <pubDate>Thu, 21 May 2020 10:19:56 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/math/</guid>
      <description>Backlinks 8 Backlinks Linear Algebra Math
Linear Algebra and its Applications Linear Algebra, Math
Bijective Math
Determinant Linear Algebra, Math
Gaussian Elimination Linear Algebra, Math
Isomorphism Math, Category Theory
Matrix Linear Algebra, Math
Morphism Math, Category Theory</description>
    </item>
    
    <item>
      <title>Neural Network</title>
      <link>https://mkschleg.github.io/braindump/neural_network/</link>
      <pubDate>Thu, 21 May 2020 10:19:56 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/neural_network/</guid>
      <description>tags Machine Learning  Backlinks 5 Backlinks ADAM Optimizers, Neural Network
LSTM Recurrent Neural Network, Neural Network, Machine Learning
Recurrent Neural Network Neural Network
ReLU Activation Neural Network
On Multiplicative Integration with Recurrent Neural Networks Recurrent Neural Network, Neural Network, Machine Learning</description>
    </item>
    
    <item>
      <title>A Common Coding Approach to Perception and Action</title>
      <link>https://mkschleg.github.io/braindump/prinz1990/</link>
      <pubDate>Thu, 21 May 2020 10:19:55 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/prinz1990/</guid>
      <description>tags Perception, Brain In this chapter, Prinz goes over evidence suggesting a shared coding between action and perception. He lays out empirical and theoretical evidence for a shared coding, and criticism for the separate coding approach. This is more directly linked to the Ideomotor Principle, and is some good evidence for having perception being dependent or apart of the behavioral responses.
  Separate Coding This view holds that the afferent and efferent codes are incommensurate at all levels of the coding.</description>
    </item>
    
    <item>
      <title>Anatomy</title>
      <link>https://mkschleg.github.io/braindump/anatomy/</link>
      <pubDate>Thu, 21 May 2020 10:19:55 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/anatomy/</guid>
      <description>Backlinks 1 Backlinks Visual System Brain, Neuroscience, Anatomy</description>
    </item>
    
    <item>
      <title>Brain</title>
      <link>https://mkschleg.github.io/braindump/brain/</link>
      <pubDate>Thu, 21 May 2020 10:19:55 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/brain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Category Theory</title>
      <link>https://mkschleg.github.io/braindump/category_theory/</link>
      <pubDate>Thu, 21 May 2020 10:19:55 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/category_theory/</guid>
      <description>Backlinks 2 Backlinks Isomorphism Math, Category Theory
Morphism Math, Category Theory</description>
    </item>
    
    <item>
      <title>How to Take Smart Notes with Org-mode</title>
      <link>https://mkschleg.github.io/braindump/how_to_take_smart_notes_with_org_mode/</link>
      <pubDate>Thu, 21 May 2020 10:19:55 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/how_to_take_smart_notes_with_org_mode/</guid>
      <description>tags Thinking, Note-Taking source How to Take Smart Notes With Org-mode | blog  This is a blog about how the author of org-roam (Jethro Kuan) takes notes and utilizes org-roam to organize his thinking.
&amp;ldquo;Note-taking is Not Just a Precursor to Writing&amp;rdquo;
Quote from Richard Feynman : &amp;ldquo;Notes aren&amp;rsquo;t a record of my thinking process. They are my thinking process.&amp;rdquo;
Process of writing:
 Find topic/research question Research/find literature Read and take notes Draw conclusions/ outline text Write  This suggests writing is linear, but infact research and writing are non-linear processes with all these ideas taking place together.</description>
    </item>
    
    <item>
      <title>On Multiplicative Integration with Recurrent Neural Networks</title>
      <link>https://mkschleg.github.io/braindump/wu2016/</link>
      <pubDate>Thu, 21 May 2020 10:19:55 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/wu2016/</guid>
      <description>tags Recurrent Neural Network, Neural Network, Machine Learning source http://papers.nips.cc/paper/6215-on-multiplicative-integration-with-recurrent-neural-networks  This paper explores a new architectural building block for recurrent neural networks, specifically one which integrates information from the internal state of the unit and the data stream through a multiplicative update. They use the Hadamard product to integrate this information together, although there are other choices here which they don&amp;rsquo;t discuss (such as various tensor products).
Additive building block:</description>
    </item>
    
    <item>
      <title>Learning to predict by the methods of temporal differences. 1988, Sutton, R.S.</title>
      <link>https://mkschleg.github.io/braindump/learning_to_predict_by_the_methods_of_temporal_differences_1988_sutton_r_s/</link>
      <pubDate>Thu, 21 May 2020 10:19:54 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/learning_to_predict_by_the_methods_of_temporal_differences_1988_sutton_r_s/</guid>
      <description>tags Reinforcement Learning source https://link.springer.com/article/10.1007/BF00115009  TLDR: This is Rich Sutton&amp;lsquo;s seminal work where he establishes Temporal Difference as a class of methods for multi-step prediction, and contrasts these methods with previous approaches. He also introduces eligibility traces and relates these to other styles of supervised learning. He then provides several examples of TD working in action. He finally proves convergence and optimality (under certain assumptions) for these types of methods.</description>
    </item>
    
    <item>
      <title>Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies</title>
      <link>https://mkschleg.github.io/braindump/chandar2019/</link>
      <pubDate>Thu, 21 May 2020 10:19:54 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/chandar2019/</guid>
      <description>tags Recurrent Neural Network, Machine Learning source https://www.aaai.org/ojs/index.php/AAAI/article/view/4200  In this paper they introduce a new recurrent cell named &amp;ldquo;NRU&amp;rdquo; for Non-saturating Recurrent Unit. There are two main contributions in this architecture which make it unique from other cells (i.e LSTM).
 The study and use of non-saturating activation functions (i.e. ReLU Activations) for the non-linear transfer functions They separate out a memory vector from the hidden state which can be a different size to the hidden state.</description>
    </item>
    
    <item>
      <title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</title>
      <link>https://mkschleg.github.io/braindump/clark2013/</link>
      <pubDate>Thu, 21 May 2020 10:19:54 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/clark2013/</guid>
      <description>tags Predictive Processing, Perception source https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/whatever-next-predictive-brains-situated-agents-and-the-future-of-cognitive-science/33542C736E17E3D1D44E8D03BE5F4CD9  Clark argues for the notion that brains are prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is inspired by the work of (Rao 1999) and others in developing the brain as a inference machine.
Introduction: Prediction Machines From Helmholtz to action-oriented predictive processing &amp;ldquo;The whole function of the brain is summed up in: error correction.</description>
    </item>
    
    <item>
      <title>Prediction, cognition and the brain</title>
      <link>https://mkschleg.github.io/braindump/bubic2010/</link>
      <pubDate>Thu, 21 May 2020 10:19:53 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/bubic2010/</guid>
      <description>tags Predictive Processing, Brain source https://www.frontiersin.org/articles/10.3389/fnhum.2010.00025/full  This paper acts as an introduction to the vast literature considering the brain as a predictive processing machine. Specifically, it is looking at terms which have been introduced such as prediction, prospection, anticipation, expectation, preparation, as well as violations of expectations or prediction errors. All these terms have different nuances and levels of abstraction associated with human behavior, and should be treated separately.</description>
    </item>
    
    <item>
      <title>Sign Theory</title>
      <link>https://mkschleg.github.io/braindump/sign_theory/</link>
      <pubDate>Thu, 21 May 2020 10:19:52 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/sign_theory/</guid>
      <description>tags Philosophy, Brain, Perception  Sign theory is a theory of perception developed by Hermann von Helmholtz of how the brain processes and propagates information (i.e. a theory of perception). The main contribution of the theory is that stimuli present themselves as signals and those signals are modified as they propagate forward. The mind makes a series of &amp;ldquo;unconscious inferences&amp;rdquo;, or mental adjustments, to construct a &amp;ldquo;picture&amp;rdquo; of the experience.</description>
    </item>
    
    <item>
      <title>Surjective</title>
      <link>https://mkschleg.github.io/braindump/surjective/</link>
      <pubDate>Thu, 21 May 2020 10:19:52 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/surjective/</guid>
      <description>Backlinks 1 Backlinks Bijective A bijection is a function between two sets where each element in the preimage has exactly one element in the image and visa versa. This means all elements in the preimage and all objects in the image have a unique pair which the function transforms from preimage set to image set. A bijective mapping is both surjective and injective.</description>
    </item>
    
    <item>
      <title>Temporal Difference Learning</title>
      <link>https://mkschleg.github.io/braindump/temporal_difference_learning/</link>
      <pubDate>Thu, 21 May 2020 10:19:52 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/temporal_difference_learning/</guid>
      <description>tags Reinforcement Learning  This is a method for learning Value Functions and was first described by (Sutton 1988).
References
(Sutton 1988) Richard S. Sutton, Learning to Predict by the Methods of Temporal Differences, Machine Learning, , pp. (1988). .
Backlinks 1 Backlinks Learning to predict by the methods of temporal differences. 1988, Sutton, R.S. TLDR*: This is Rich Sutton&amp;lsquo;s seminal work where he establishes Temporal Difference as a class of methods for multi-step prediction, and contrasts these methods with previous approaches.</description>
    </item>
    
    <item>
      <title>Value Function</title>
      <link>https://mkschleg.github.io/braindump/value_function/</link>
      <pubDate>Thu, 21 May 2020 10:19:52 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/value_function/</guid>
      <description>tags Reinforcement Learning  Backlinks 1 Backlinks Temporal Difference Learning This is a method for learning Value Functions and was first described by sutton1988.</description>
    </item>
    
    <item>
      <title>Visual System</title>
      <link>https://mkschleg.github.io/braindump/visual_system_/</link>
      <pubDate>Thu, 21 May 2020 10:19:52 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/visual_system_/</guid>
      <description>tags Brain, Neuroscience, Anatomy source https://nba.uth.tmc.edu/neuroscience/s2/chapter14.html  Components    Backlinks 1 Backlinks Whatever next? Predictive brains, situated agents, and the future of cognitive science hosoya2005&amp;lsquo;s hosoya2005account of dynamic predictive coding moves context center states. The neural circuits predict, on the basis of local image characteristics, the likely image characteristics of nearby spots in space and time. The encoded value is not the raw image data, but the residual or departures from the predictable structure.</description>
    </item>
    
    <item>
      <title>Zettelkasten method</title>
      <link>https://mkschleg.github.io/braindump/zettelkasten_method/</link>
      <pubDate>Thu, 21 May 2020 10:19:52 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/zettelkasten_method/</guid>
      <description>tags Note-Taking source zettelkasten.de  This is a method for note taking developed by Niklas Luhmann. There are several principles under which the method derives it&amp;rsquo;s workflow:
 Your notes should be atomic: This means topics should be placed into a single note, butthe scope of the topic should be small. This makes it easier to link and categorize throughout your notes. Your notes (or collection of notes) should be interconnected: This means you should continually make connections with notes previously written and place new notes into this structure.</description>
    </item>
    
    <item>
      <title>Morphism</title>
      <link>https://mkschleg.github.io/braindump/morphism/</link>
      <pubDate>Thu, 21 May 2020 10:19:51 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/morphism/</guid>
      <description>tags Math, Category Theory source https://en.wikipedia.org/wiki/Morphism  A morphism is a map between mathematical objects of the same type. This map is structure preserving.
Backlinks 1 Backlinks Isomorphism definition*: An isomorphism is a mapping between two mathematical groups which maintains sets and relations of the elements of the groups. If two mathematical objects have an isomorphism this means they are effectively the same objects after all the elements are renamed (through the isomorph mapping).</description>
    </item>
    
    <item>
      <title>Preimage</title>
      <link>https://mkschleg.github.io/braindump/preimage/</link>
      <pubDate>Thu, 21 May 2020 10:19:51 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/preimage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recurrent Neural Network</title>
      <link>https://mkschleg.github.io/braindump/recurrent_neural_network/</link>
      <pubDate>Thu, 21 May 2020 10:19:51 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/recurrent_neural_network/</guid>
      <description>tags Neural Network  See other extensions: LSTM, (Wu 2016), (Chandar 2019)
(Wu 2016) Yuhuai Wu; Saizheng Zhang; Ying Zhang; Yoshua Bengio and Russ R Salakhutdinov, On {{Multiplicative Integration}} with {{Recurrent Neural Networks}}, (2016).
(Chandar 2019) Sarath Chandar; Chinnadhurai Sankar; Eugene Vorontsov; Samira Ebrahimi Kahou and Yoshua Bengio, Towards {{Non}}-Saturating {{Recurrent Units}} for {{Modelling Long}}-Term {{Dependencies}}, {{AAAI}}(2019).
Backlinks 4 Backlinks LSTM Recurrent Neural Network, Neural Network, Machine Learning
Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies Recurrent Neural Network, Machine Learning</description>
    </item>
    
    <item>
      <title>ReLU Activation</title>
      <link>https://mkschleg.github.io/braindump/relu_activation/</link>
      <pubDate>Thu, 21 May 2020 10:19:51 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/relu_activation/</guid>
      <description>tags Neural Network  A simple activation function which clips the output of the internal operations to be positive.
\[ f(\mathbf{x}_i) = \begin{cases} \mathbf{x}_i, &amp;amp; \text{ for } \mathbf{x_i} \ge 0 \\\
0 &amp;amp; \text{ o.w.} \end{cases} \]
The main hallmark of this activation function is the smaller effect of the vanishing gradient problem in deep neural networks. This comes at a cost of stability and the greater risk of dead neurons.</description>
    </item>
    
    <item>
      <title>Rich Sutton</title>
      <link>https://mkschleg.github.io/braindump/rich_sutton/</link>
      <pubDate>Thu, 21 May 2020 10:19:51 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/rich_sutton/</guid>
      <description>tags People, Reinforcement Learning website http://incompleteideas.net  Backlinks 1 Backlinks Learning to predict by the methods of temporal differences. 1988, Sutton, R.S. TLDR*: This is Rich Sutton&amp;lsquo;s seminal work where he establishes Temporal Difference as a class of methods for multi-step prediction, and contrasts these methods with previous approaches. He also introduces eligibility traces and relates these to other styles of supervised learning. He then provides several examples of TD working in action.</description>
    </item>
    
    <item>
      <title>Richard Feynman</title>
      <link>https://mkschleg.github.io/braindump/richard_feynman/</link>
      <pubDate>Thu, 21 May 2020 10:19:51 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/richard_feynman/</guid>
      <description>Backlinks 1 Backlinks How to Take Smart Notes with Org-mode Quote from Richard Feynman : &amp;ldquo;Notes aren&amp;rsquo;t a record of my thinking process. They are my thinking process.&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Image</title>
      <link>https://mkschleg.github.io/braindump/image/</link>
      <pubDate>Thu, 21 May 2020 10:19:50 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/image/</guid>
      <description>Backlinks 2 Backlinks Bijective A bijection is a function between two sets where each element in the preimage has exactly one element in the image and visa versa. This means all elements in the preimage and all objects in the image have a unique pair which the function transforms from preimage set to image set. A bijective mapping is both surjective and injective.
A bijection is a function between two sets where each element in the preimage has exactly one element in the image and visa versa.</description>
    </item>
    
    <item>
      <title>Injective</title>
      <link>https://mkschleg.github.io/braindump/injective/</link>
      <pubDate>Thu, 21 May 2020 10:19:50 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/injective/</guid>
      <description>Backlinks 1 Backlinks Bijective A bijection is a function between two sets where each element in the preimage has exactly one element in the image and visa versa. This means all elements in the preimage and all objects in the image have a unique pair which the function transforms from preimage set to image set. A bijective mapping is both surjective and injective.</description>
    </item>
    
    <item>
      <title>Isomorphism</title>
      <link>https://mkschleg.github.io/braindump/isomorph/</link>
      <pubDate>Thu, 21 May 2020 10:19:50 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/isomorph/</guid>
      <description>tags Math, Category Theory  definition: An isomorphism is a mapping between two mathematical groups which maintains sets and relations of the elements of the groups. If two mathematical objects have an isomorphism this means they are effectively the same objects after all the elements are renamed (through the isomorph mapping). More formally an isomorphism is a bijective morphism.
Backlinks 1 Backlinks Whatever next? Predictive brains, situated agents, and the future of cognitive science Isomorphism: any mathematical mapping (morphism) which can be inversed through an inverse morphism.</description>
    </item>
    
    <item>
      <title>LSTM</title>
      <link>https://mkschleg.github.io/braindump/lstm/</link>
      <pubDate>Thu, 21 May 2020 10:19:50 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/lstm/</guid>
      <description>tags Recurrent Neural Network, Neural Network, Machine Learning source https://colah.github.io/posts/2015-08-Understanding-LSTMs/  Backlinks 2 Backlinks Recurrent Neural Network See other extensions: LSTM, wu2016, chandar2019Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies In this paper they introduce a new recurrent cell named &amp;ldquo;NRU&amp;rdquo; for Non-saturating Recurrent Unit. There are two main contributions in this architecture which make it unique from other cells (i.e LSTM).</description>
    </item>
    
    <item>
      <title>Matrix</title>
      <link>https://mkschleg.github.io/braindump/matrix/</link>
      <pubDate>Thu, 21 May 2020 10:19:50 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/matrix/</guid>
      <description>tags Linear Algebra, Math  We star with a set of equations with nine coefficients, three unknowns, and three right-hand sides.
\begin{align*} 2u + v + w &amp;amp;= 5 \\\
4u - 6v &amp;amp;=-2 \\\
-2u + 7v + 2w &amp;amp;=9 \end{align*}
We represent the right hand side with a column vector (or a 1 by 3 matrix). The unknowns of the system are represented as a column vector as well.</description>
    </item>
    
    <item>
      <title>Efferent Codes</title>
      <link>https://mkschleg.github.io/braindump/efferent_codes/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/efferent_codes/</guid>
      <description>Backlinks 1 Backlinks A Common Coding Approach to Perception and Action Efferent Codes the actions (act, react, behavior&amp;hellip;)</description>
    </item>
    
    <item>
      <title>Free-Energy Principle</title>
      <link>https://mkschleg.github.io/braindump/free_energy_principle/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/free_energy_principle/</guid>
      <description>tags Predictive Processing,  Backlinks 1 Backlinks Whatever next? Predictive brains, situated agents, and the future of cognitive science Free-Energy Principle: all the quantities that can change; i.e. that are part of the system, will change to minimize free-energy.</description>
    </item>
    
    <item>
      <title>Gaussian Elimination</title>
      <link>https://mkschleg.github.io/braindump/gaussian_elimination/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/gaussian_elimination/</guid>
      <description>tags Linear Algebra, Math  Elimination is all about solving linear equations (which is central to linear algebra generally). As an example we have two equations with two unknowns
\begin{align*} 1x + 2y = 3 \\\
4x + 5y = 6 \end{align*} (from [[file:../linear_algebra_and_its_applications.org][Linear Algebra and its Applications]])
To solve for the unknowns $x$ and $y$ we can use elimination:
  subtract 4 times the first equation from the second equation (eliminates $x$ from the second equation)</description>
    </item>
    
    <item>
      <title>Hadamard product</title>
      <link>https://mkschleg.github.io/braindump/hadamard_product/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/hadamard_product/</guid>
      <description>tags Linear Algebra  This is an element wise product between two vectors or matrices.
It is quite simply the element-wise produce between two matrices: \(\mathcal{A}, \mathcal{B}: n \times m\).
\begin{align*} \left[\begin{array}{cc} {A_{11}} &amp;amp; {A_{12}} \\\
{A_{21}} &amp;amp; {A_{22}} \\\
{A_{31}} &amp;amp; {A_{32}} \end{array}\right] \odot \left[\begin{array}{cc} {B_{11}} &amp;amp; {B_{12}} \\\
{B_{21}} &amp;amp; {B_{22}} \\\
{B_{31}} &amp;amp; {B_{32}} \end{array}\right] &amp;amp;= \left[\begin{array}{ll} {A_{11}B_{11}} &amp;amp; {A_{12}B_{12}} \\\
{A_{21}B_{21}} &amp;amp; {A_{22}B_{22}} \\\
{A_{31}B_{31}} &amp;amp; {A_{32}B_{32}} \end{array}\right] \\\</description>
    </item>
    
    <item>
      <title>Hermann von Helmholtz</title>
      <link>https://mkschleg.github.io/braindump/hermann_von_helmholtz/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/hermann_von_helmholtz/</guid>
      <description>tags People source https://plato.stanford.edu/entries/hermann-helmholtz/  Backlinks 3 Backlinks Sign Theory Sign theory is a theory of perception developed by Hermann von Helmholtz of how the brain processes and propagates information (i.e. a theory of perception). The main contribution of the theory is that stimuli present themselves as signals and those signals are modified as they propagate forward. The mind makes a series of &amp;ldquo;unconscious inferences&amp;rdquo;, or mental adjustments, to construct a &amp;ldquo;picture&amp;rdquo; of the experience.</description>
    </item>
    
    <item>
      <title>Hierarchical Predictive Coding</title>
      <link>https://mkschleg.github.io/braindump/hierarchical_predictive_coding/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/hierarchical_predictive_coding/</guid>
      <description>Backlinks 3 Backlinks Whatever next? Predictive brains, situated agents, and the future of cognitive science This work by brown2011generalizes the basic Hierarchical Predictive Coding model to include action. The fundamental attraction of these accounts lies in their ability to offer a deeply unified account of perception, cognition, and action.
The strategy of using top-down connections to try to generate, using high-level knowledge, a kind of &amp;ldquo;virtual version&amp;rdquo; of the sensory data via a deep multilevel cascade lies at the heart of Hierarchical Predictive Coding rao1999.</description>
    </item>
    
    <item>
      <title>Ideomotor Principle</title>
      <link>https://mkschleg.github.io/braindump/ideomotor_principle/</link>
      <pubDate>Thu, 21 May 2020 10:19:49 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/ideomotor_principle/</guid>
      <description>Backlinks 1 Backlinks A Common Coding Approach to Perception and Action In this chapter, Prinz goes over evidence suggesting a shared coding between action and perception. He lays out empirical and theoretical evidence for a shared coding, and criticism for the separate coding approach. This is more directly linked to the Ideomotor Principle, and is some good evidence for having perception being dependent or apart of the behavioral responses.</description>
    </item>
    
    <item>
      <title>ADAM</title>
      <link>https://mkschleg.github.io/braindump/adam/</link>
      <pubDate>Thu, 21 May 2020 10:19:48 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/adam/</guid>
      <description>tags Optimizers, Neural Network  An optimizer which effectively combines RMSProp and Momentum.
Backlinks 1 Backlinks On Multiplicative Integration with Recurrent Neural Networks First they look at the gradients when the RNNs have linear activation mappings (to focus on the internal mechanisms). They measure the log of the L2-norm of the gradient for each epoch (averaged over the training set) using the Penn-Treebank dataset using the ADAM optimizer. They are able to show the norm of the gradient grows much more in vanilla architecture (using additive operations) vs what occurs in the new architecture.</description>
    </item>
    
    <item>
      <title>Afferent Codes</title>
      <link>https://mkschleg.github.io/braindump/afferent_codes/</link>
      <pubDate>Thu, 21 May 2020 10:19:48 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/afferent_codes/</guid>
      <description>tags Brain, Neuroscience  Backlinks 1 Backlinks A Common Coding Approach to Perception and Action Afferent Codes The perceptions (recieve, input, sense&amp;hellip;)</description>
    </item>
    
    <item>
      <title>Bijective</title>
      <link>https://mkschleg.github.io/braindump/bijective/</link>
      <pubDate>Thu, 21 May 2020 10:19:48 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/bijective/</guid>
      <description>tags Math source https://en.wikipedia.org/wiki/Bijection  A bijection is a function between two sets where each element in the preimage has exactly one element in the image and visa versa. This means all elements in the preimage and all objects in the image have a unique pair which the function transforms from preimage set to image set. A bijective mapping is both surjective and injective.
Backlinks 1 Backlinks Isomorphism definition*: An isomorphism is a mapping between two mathematical groups which maintains sets and relations of the elements of the groups.</description>
    </item>
    
    <item>
      <title>Binocular rivalry</title>
      <link>https://mkschleg.github.io/braindump/binocular_rivalry/</link>
      <pubDate>Thu, 21 May 2020 10:19:48 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/binocular_rivalry/</guid>
      <description>tags Brain, Perception source https://www.sciencedirect.com/topics/neuroscience/binocular-rivalry  Also called Binocular contour rivalry
Discovered by Porta in 1593 (Wade 1996). The essence is the interplay of perception with vision when you separate the visual stimuli of each eye. Each eye is presented with a separate image with a partition separating the effective visual field. A continuous cycle of the two images being recognizable (each in its own turn) with brief periods of mixed forms appearing.</description>
    </item>
    
    <item>
      <title>Determinant</title>
      <link>https://mkschleg.github.io/braindump/determinant/</link>
      <pubDate>Thu, 21 May 2020 10:19:48 -0600</pubDate>
      
      <guid>https://mkschleg.github.io/braindump/determinant/</guid>
      <description>tags Linear Algebra, Math  Uses There are four main uses for the determinant of a Matrix defined by a series of equations.
 They test for invertibility. If the determinant of A is zero, then A is singular. If the determinant is non-zero than the matrix can be inverted, meaning there is at least one solution to the set of equations. The determinant of a matrix is the volume of a box in n-dimensional space.</description>
    </item>
    
    <item>
      <title>Raving for Rave Coffee</title>
      <link>https://mkschleg.github.io/post/2019-02-24-canmore-rave-coffe/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/post/2019-02-24-canmore-rave-coffe/</guid>
      <description>A caffeine induced rave about Rave</description>
    </item>
    
    <item>
      <title>Creed(o) of Procrastination</title>
      <link>https://mkschleg.github.io/post/2018-11-13-credo-procrastination/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/post/2018-11-13-credo-procrastination/</guid>
      <description>Ok. Look. I procrastinate. I procrastinate a lot. I have a reasonably sized talk tomorrow at Startup Edmonton (you should come! Details found here!), of which I have yet to finish, let alone practice. So instead of doing that I&amp;rsquo;ve decided to come and enjoy the ritual of an espresso at Credo Coffee. I&amp;rsquo;m not proud of my choices, but they are the choices that get made (either by me or at the whims of some biological process).</description>
    </item>
    
    <item>
      <title>Why am I blogging?</title>
      <link>https://mkschleg.github.io/post/2018-11-03-blog-state-of-purpose/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/post/2018-11-03-blog-state-of-purpose/</guid>
      <description>So why am I starting a blog? And what is it going to be about? These are great questions, and hopefully this post will act as a launching pad for the content that will eventually be found here. Instead of having a long rant up front, I&amp;rsquo;m going to answer a bunch of questions you may have and you can easily skip sections if they are uninteresting.
Who the heck are you?</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://mkschleg.github.io/about/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/about/</guid>
      <description>I&amp;rsquo;m a PhD student at University of Alberta in Edmonton, Alberta, Canada. I have a BS in Physics and an MS in Computer Science both from Indiana University Bloomington. My current PhD work is focused on reinforcement learning, and specifically in understanding how agents may perceive their world. I focus primarily on prediction making, but have been known to dabble in control from time-to-time. My active research interests include: predictions as a component in intelligence (both artificial and biological), off-policy prediction and policy evaluation, deep learning and resulting learned representations in the reinforcement learning context, and discovery or attention of important abstractions (described as predictions) through interaction.</description>
    </item>
    
    <item>
      <title>CV</title>
      <link>https://mkschleg.github.io/cv/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://mkschleg.github.io/publications/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/publications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visited Coffee Locations</title>
      <link>https://mkschleg.github.io/coffee/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mkschleg.github.io/coffee/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>