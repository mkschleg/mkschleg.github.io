<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <meta name="robots" content="noindex">
  <meta name="googlebot" content="noindex">
  
  
  <meta name="generator" content="Hugo 0.91.2" />
  <meta name="author" content="Matthew Schlegel">

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,700%7cOpen&#43;Sans:400,400italic,700%7cRoboto&#43;Mono%25!%28EXTRA%20*hugolib.pageState=Page%28/braindump/causality.md%29%29">
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/header.css">
  
  <link rel="stylesheet" href="/css/img.css">
  
  <link rel="stylesheet" href="/css/braindump.css">
  
  <link rel="stylesheet" href="/css/post.css">
  

  

  

  <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon/favicon-16x16.png">
  <link rel="manifest" href="/img/favicon/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://mkschleg.github.io/braindump/causality/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@mattschleg">
  <meta property="twitter:creator" content="@mattschleg">
  
  <meta property="og:site_name" content="Matthew Schlegel">
  <meta property="og:url" content="https://mkschleg.github.io/braindump/causality/">
  <meta property="og:title" content="Causality | Matthew Schlegel">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2021-10-11T11:31:35-04:00">
  <meta property="article:modified_time" content="2021-10-11T11:31:35-04:00">
  

  <title>Causality | Matthew Schlegel</title>

  

  
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']], 
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

</head>
<body>

<style type="text/css">
  
 
  
 
</style>

<div class="masthead-hero"></div>


<div id="main" role="main">
  <div class="sidebar sticky" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <div class="author-avatar">
    <a href="/">
      
      <img src="/img/me.jpg" alt="Matthew Schlegel" itemprop="image">
      
    </a>
  </div>
  <div class="author-content">
    <h3 class="author-name" itemprop="name">Matthew Schlegel</h3>
    <p class="author-bio" itemprop="description">Lover of Espresso; PhD student at Amii, RLAI and UofA; Works on how machines perceive their world.</p>
  </div>
  <div class="author-urls-wrapper">
    <ul class="author-urls social-icons" aria-hidden="true">
      <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
        <span itemprop="name">Edmonton, Alberta</span>
      </li>
      
      <li>
        <a itemprop="sameAs" href="//linkedin.com/in/mkschleg" target="_blank" rel="noopener noreferrer">
          <i class="fa fa-linkedin"></i>
          LinkedIn
        </a>
      </li>
      
      <li>
        <a itemprop="sameAs" href="//twitter.com/mattschleg" target="_blank" rel="noopener noreferrer">
          <i class="fa fa-twitter"></i>
          Twitter
        </a>
      </li>
      
      <li>
        <a itemprop="sameAs" href="//github.com/mkschleg" target="_blank" rel="noopener noreferrer">
          <i class="fa fa-github"></i>
          Github
        </a>
      </li>
      
    </ul>
    <ul class="author-urls social-icons" aria-hidden="true" style="margin-top:30px;">
      
      <li>
        <a itemprop="sameAs" href=/tags >
          <i class="fa fa-tag"></i>
          Tags
        </a>
      </li>
      
      <li>
        <a itemprop="sameAs" href=/collections >
          <i class="fa fa-folder"></i>
          Collections
        </a>
      </li>
      
    </ul>
  </div>
</div>

  <article class="page">
		<div class="page_container">
			<section class="page_content">
				<div class="navbar-hero">
  <nav>
    
    
    <a class="hover" href="/">Home</a>
    
    
    <a class="hover" href="/about">About</a>
    
    
    <a class="hover" href="/publications">Publications</a>
    
    
    <a class="hover" href="/post">Posts</a>
    
    
    <a class="hover" href=/braindump> BrainDump </a>
    
  </nav>
</div>

				<article class="post" itemscope itemtype="http://schema.org/Article">
  <div class="post-container">
    <h1 itemprop="name"><a href="https://mkschleg.github.io/braindump/causality/">Causality</a></h1>

    
      

<div class="post-metadata">

  <span class="post-date">
    
    <time datetime="2021-10-11 11:31:35 -0400 -0400" itemprop="datePublished dateModified">
      Oct 11, 2021
    </time>
  </span>

  

</div>

    

    <div class="post-style" itemprop="articleBody">
      
      <p>\( \newcommand{\states}{\mathcal{S}}
\newcommand{\actions}{\mathcal{A}}
\newcommand{\observations}{\mathcal{O}}
\newcommand{\rewards}{\mathcal{R}}
\newcommand{\traces}{\mathbf{e}}
\newcommand{\transition}{P}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\expected}{\mathbb{E}}
\newcommand{\by}{\times}
\newcommand{\partialderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\defineq}{\stackrel{{\tiny\mbox{def}}}{=}}
\newcommand{\defeq}{\stackrel{{\tiny\mbox{def}}}{=}}
\newcommand{\eye}{\Imat}
\newcommand{\hadamard}{\odot}
\newcommand{\trans}{\top}
\newcommand{\inv}{{-1}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\avec}{\mathbf{a}}
\newcommand{\bvec}{\mathbf{b}}
\newcommand{\cvec}{\mathbf{c}}
\newcommand{\dvec}{\mathbf{d}}
\newcommand{\evec}{\mathbf{e}}
\newcommand{\gvec}{\mathbf{g}}
\newcommand{\hvec}{\mathbf{h}}
\newcommand{\lvec}{\mathbf{l}}
\newcommand{\mvec}{\mathbf{m}}
\newcommand{\nvec}{\mathbf{n}}
\newcommand{\pvec}{\mathbf{p}}
\newcommand{\qvec}{\mathbf{q}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\svec}{\mathbf{s}}
\newcommand{\uvec}{\mathbf{u}}
\newcommand{\vvec}{\mathbf{v}}
\newcommand{\wvec}{\mathbf{w}}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\yvec}{\mathbf{y}}
\newcommand{\zvec}{\mathbf{z}}
\newcommand{\Amat}{\mathbf{A}}
\newcommand{\Bmat}{\mathbf{B}}
\newcommand{\Cmat}{\mathbf{C}}
\newcommand{\Dmat}{\mathbf{D}}
\newcommand{\Emat}{\mathbf{E}}
\newcommand{\Fmat}{\mathbf{F}}
\newcommand{\Imat}{\mathbf{I}}
\newcommand{\Pmat}{\mathbf{P}}
\newcommand{\Umat}{\mathbf{U}}
\newcommand{\Vmat}{\mathbf{V}}
\newcommand{\Wmat}{\mathbf{W}}
\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\Qmat}{\mathbf{Q}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\phivec}{\boldsymbol{\phi}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\sigmavec}{\boldsymbol{\sigma}}
\newcommand{\jacobian}{\mathbf{J}}
\newcommand{\ind}{\perp!!!!\perp}
\)</p>
<p>This note will talk about the <a href="/braindump/statistics/">Statistics</a> and <a href="/braindump/artificial_intelligence/">AI</a> notions of causality. This will also host as a place to link to various resources and notes on these resources.</p>
<h2 id="common-cause-principle">Common Cause principle</h2>
<p>It is well known that statistical properties alone do not determine causal structures.</p>
<p><a id="org792ea8c"></a></p>
<figure><img src="/ox-hugo/reichenbach_ccp_fig.png"
         alt="Figure 1: Reichenbach&amp;rsquo;s common cause principle establishes a link between statistical properties and causal structures. A statistical dependence between two observables \(X\) and \(Y\) indicates taht they are caused by a (potentially new) variable \(Z\). In the figures cause is denoted through arrows."/><figcaption>
            <p>Figure 1: Reichenbach&rsquo;s common cause principle establishes a link between statistical properties and causal structures. A statistical dependence between two observables \(X\) and \(Y\) indicates taht they are caused by a (potentially new) variable \(Z\). In the figures cause is denoted through arrows.</p>
        </figcaption>
</figure>

<div class="principle">
  <div></div>
<div class="title">
  <div></div>
<p>Reichenbach&rsquo;s common cause principle:</p>
</div>
<p>If two random variables X and Y are statistically dependent, then there exists a third variable Z that causally influences both. (As a special case, Z may coincide with either X or Y.) Furthermore, this variable Z screens X and Y from each other in the sense that given Z, they become independent.</p>
</div>
<p>While this principle lays out a primal causal model, estimating such a model from data (especially if the data isn&rsquo;t <a href="#causal-sufficiency">causally sufficient</a>) is extremely difficult and in many cases the model is not identifiable. While this should give us pause into the motivations of using causality in machine learning, making assumptions about the data generation process without hard assumptions on the agent&rsquo;s used internal <a href="#causal-model">Causal Model</a> might produce powerful techniques or insights into designing algorithms. This direction might also provide groundwork for understanding when problems in perception are hopeless <sup id="b00c0ffe2b498797f6925d0886d290da"><a href="#scholkopf" title="Sch\olkopf, Janzing, Peters, Sgouritsa, Zhang \&amp; Mooij, On {{Causal}} and {{Anticausal Learning}}, v(), ().">scholkopf</a></sup>. With this in mind we will cautiously move forward.</p>
<h2 id="terms-and-special-cases">Terms and special cases</h2>
<h3 id="disentangled-model">Disentangled model</h3>
<p><a href="https://arxiv.org/abs/2102.11107">https://arxiv.org/abs/2102.11107</a></p>
<h3 id="independent-causal-mechanisms-principle">Independent Causal Mechanisms Principle</h3>
<h3 id="markov-factorizations">Markov Factorizations</h3>
<h3 id="domain-adaptation">Domain adaptation</h3>
<h3 id="classes-of-causal-models">Classes of Causal models</h3>
<h3 id="causal-hierarchy">Causal hierarchy</h3>
<h2 id="some-assumptions-for-tractability">Some assumptions for tractability</h2>
<p>These are defined under a simple functional causal model where \(C\) is a cause variable (with id noise \(N_C\)), the function \(\psi\) is a deterministic mechanism, and \(E = \psi(C, N_E)\) with \(N_E\) is independent noise.</p>
<h3 id="causal-sufficiency">Causal Sufficiency</h3>
<p>Assuming two independent noise variables \(N_C\) and \(N_E\) with random variables with distributions \(P(N_C)\) and \(P(N_E)\). The function \(\phi\) and the noise \(N_E\) jointly determine \(P(E|C)\) via \(E=\phi(C, N_E)\). This conditional is thought as the mechanism transforming cause \(C\) into effect \(E\).</p>
<h3 id="independence-of-mechanism-and-input">Independence of mechanism and input</h3>
<p>The mechanism \(\psi\) is independent of the distribution of the cause.</p>
<h3 id="richness-of-functional-causal-models">Richness of functional causal models</h3>
<p>Two-variable functional causal models are so &ldquo;rich&rdquo; that the causal direction cannot be inferred.</p>
<h3 id="additive-noise-models">Additive noise models</h3>
<p>The additive noise model assumes for some function \(\phi\)</p>
<p>\[ E = \phi( C) + N_E \]</p>
<p>The importance of this is that under usual conditions (i.e. if \(\phi\) is linear and \(N_E\) was gaussian), two real-valued random variables X and Y can be fit by an ANM model in at most one direction (which is considered the causal direction).</p>
<h3 id="the-causal-markov-condition">The Causal Markov condition</h3>
<p>Let \(G\) be a causal graph with vertices \(\mathbf{V}\) and \(P\) be a probability distribution over the vertices in \(\mathbf{V}\) generated by the causal structure represented by \(G\). \(G\) and \(P\) satisfy the causal markov condition if and only if for every \(W \in \mathbf{V}\), \(W\) is independent of \(V(\text{\bf Descendants}(W) \cup \text{\bf Parents}(W))\) given \(\text{\bf Parents}(W)\).</p>
<h3 id="the-causal-minimality-condition">The Causal Minimality Condition</h3>
<h3 id="the-faithfulness-condition">The Faithfulness Condition</h3>
<h2 id="causal-model">Causal Model</h2>
<h2 id="as-it-relates-to-time-series">As it relates to time series</h2>
<h3 id="granger-causality">Granger Causality</h3>
<h2 id="questions">Questions</h2>
<h3 id="inferring-the-causal-model-from-data-dot">Inferring the causal model from data.</h3>
<p>Say we observe the data from the toy examples in inference&rsquo;s blog. How do you disentangle this to find the causal mechanisms? Are you able to run something through the function \(\phi\)? This seem unrealistic, especially if you <em>just</em> have the data&hellip;</p>
<h3 id="counterfactuals-aren-t-testable">Counterfactuals aren&rsquo;t testable</h3>
<p>If we are using a causal model to build some type of way to do counterfactual reasoning, how can we measure performance of models? In toy domains it is easy to do generate the correct distributions, but my time in RL has taught me that nothing is so simple when moving beyond toy problems. (i.e. distribution shifts can have major implications for the space the agent is in vs where it has seen before). Acting in the world begets distributional change, which effects the original problem. Does causal inference reason about these distributional shifts? What are the knock on effects?</p>
<h3 id="inferring-which-causal-model-is-correct-from-data">Inferring which causal model is correct from data</h3>
<p>Given the toy examples and two causal models, can we infer which model is correct given the observed data. I suspect the answer to this is no, and we need to do interventions to uncover the actual causal model. But then I&rsquo;m confused by the motivation. We are motivated to learn the causal structure of a problem w/o being able to do A/B testing&hellip; If we can&rsquo;t uncover the causal structure w/o intervention and A/B testing, then what is the point of causal inference?</p>
<p>Apparently this can be done w/ additive noise models and nonlinear causal relationships with an extremely simple mechanism described by <sup id="a94adfeafb37f4427d6434d2810495ef"><a href="#hoyer2009" title="Hoyer, Janzing, Mooij, Peters \&amp; Sch\olkopf, Nonlinear Causal Discovery with Additive Noise Models, in in: {Advances in {{Neural Information Processing Systems}}}, edited by {Curran Associates, Inc.} (2009)">hoyer2009</a></sup>. (i.e. do n regressions and check the causal relationship based on the correlation with the resulting irreducible error).</p>
<h3 id="is-the-independence-of-mechanism-and-input-a-reasonable-assumption-for-learning-systems">Is the independence of mechanism and input a reasonable assumption for learning systems?</h3>
<p>I&rsquo;m reminded of weapons of math destruction. Can we assume the underlying mechanisms for generating effects are independent of the distributions we use? I guess you can always separate out the different functions for different variables, i.e. instead of \(C \rightarrow_\psi E\) you have \((C_1, C_2, C_3, \ldots) \rightarrow_\psi E\). So in this instance we will have a multi-input causal structure.</p>
<h3 id="is-the-restriction-to-non-linear-w-additive-gaussian-noise-true-or-is-it-just-invertability-of-the-causal-mechanism">Is the restriction to non-linear w/ additive gaussian noise true, or is it just invertability of the causal mechanism?</h3>
<h3 id="can-we-infer-between-various-causal-graphs-with-un-seen-latent-causes">Can we infer between various causal graphs with un-seen latent causes?</h3>
<h3 id="how-does-the-causal-inference-in-depend-on-model-performance">How does the causal inference in <sup id="a94adfeafb37f4427d6434d2810495ef"><a href="#hoyer2009" title="Hoyer, Janzing, Mooij, Peters \&amp; Sch\olkopf, Nonlinear Causal Discovery with Additive Noise Models, in in: {Advances in {{Neural Information Processing Systems}}}, edited by {Curran Associates, Inc.} (2009)">hoyer2009</a></sup> depend on model performance?</h3>
<h3 id=""></h3>
<h2 id="causal-state-representations">Causal State Representations</h2>
<ul>
<li><sup id="cf8e9083815e9b7d8318497cd8335abc"><a href="#zhang2021" title="Zhang, Lipton, Pineda, Azizzadenesheli, Anandkumar, Itti, Pineau \&amp; Furlanello, Learning {{Causal State Representations}} of {{Partially Observable Environments}}, {arXiv:1906.10437 [cs, stat]}, v(), (2021).">zhang2021</a></sup></li>
</ul>
<h2 id="reading-list">Reading list</h2>
<h3 id="causal-state-representations">Causal state representations</h3>
<p><a href="https://arxiv.org/pdf/1906.10437.pdf">https://arxiv.org/pdf/1906.10437.pdf</a></p>
<h3 id="learning-causal-state-representations-of-partially-observable-environments">Learning Causal State Representations of Partially Observable Environments</h3>
<p><a href="https://arxiv.org/abs/2102.11107">https://arxiv.org/abs/2102.11107</a></p>
<h3 id="causal-effect-identifiability-under-partial-observability">Causal Effect Identifiability under Partial-Observability</h3>
<p><a href="http://proceedings.mlr.press/v119/lee20a.html">http://proceedings.mlr.press/v119/lee20a.html</a></p>
<h3 id="https-arxiv-dot-org-abs-2106-dot-04619"><a href="https://arxiv.org/abs/2106.04619">https://arxiv.org/abs/2106.04619</a></h3>
<h3 id="nonlinear-causal-discovery-with-additive-noise-models">Nonlinear causal discovery with additive noise models</h3>
<p><a href="https://papers.nips.cc/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf">https://papers.nips.cc/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf</a></p>
<h3 id="granger-causality">Granger Causality</h3>
<h3 id=""><sup id="ddc8f7a086caeb6d5bd95f9b7cdc5334"><a href="#scholkopf2019" title="Scholkopf, Causality for {{Machine Learning}}, {arXiv:1911.10500 [cs, stat]}, v(), (2019).">scholkopf2019</a></sup></h3>
<h2 id="references">References</h2>
<h1 id="bibliography">Bibliography</h1>
<p><a id="scholkopf"></a>[scholkopf] Sch&quot;olkopf, Janzing, Peters, Sgouritsa, Zhang &amp; Mooij, On Causal and Anticausal Learning, <i></i>,  . <a href="#b00c0ffe2b498797f6925d0886d290da">↩</a></p>
<p><a id="hoyer2009"></a>[hoyer2009] Hoyer, Janzing, Mooij, Peters &amp; Sch&quot;olkopf, Nonlinear Causal Discovery with Additive Noise Models, in in: Advances in Neural Information Processing Systems, edited by Curran Associates, Inc. (2009) <a href="#a94adfeafb37f4427d6434d2810495ef">↩</a></p>
<p><a id="zhang2021"></a>[zhang2021] Zhang, Lipton, Pineda, Azizzadenesheli, Anandkumar, Itti, Pineau &amp; Furlanello, Learning Causal State Representations of Partially Observable Environments, <i>arXiv:1906.10437 [cs, stat]</i>,  (2021). <a href="#cf8e9083815e9b7d8318497cd8335abc">↩</a></p>
<p><a id="scholkopf2019"></a>[scholkopf2019] Scholkopf, Causality for Machine Learning, <i>arXiv:1911.10500 [cs, stat]</i>,  (2019). <a href="#ddc8f7a086caeb6d5bd95f9b7cdc5334">↩</a></p>

    </div>

    
    

    

    
    

    

    

    

  </div>
</article>

			</section>
		</div>
	</article>
</div> 


<div class="page_footer">
	<p>Copyright © 2020 Matthew Schlegel. All Rights Reserved. Powered by <a href="http://gohugo.io/">Hugo</a> and <a href="https://github.com/jhu247/minimal-academic">Minimal Academic</a>.</p>
</div>
    
    


  </body>
</html>
