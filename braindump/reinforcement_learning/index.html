<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <meta name="robots" content="noindex">
  <meta name="googlebot" content="noindex">
  
  
  <meta name="generator" content="Hugo 0.102.1" />
  <meta name="author" content="Matthew Schlegel">

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,700%7cOpen&#43;Sans:400,400italic,700%7cRoboto&#43;Mono%25!%28EXTRA%20*hugolib.pageState=Page%28/braindump/reinforcement_learning.md%29%29">
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/header.css">
  
  <link rel="stylesheet" href="/css/img.css">
  
  <link rel="stylesheet" href="/css/braindump.css">
  
  <link rel="stylesheet" href="/css/post.css">
  

  

  

  <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon/favicon-16x16.png">
  <link rel="manifest" href="/img/favicon/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://mkschleg.github.io/braindump/reinforcement_learning/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@mattschleg">
  <meta property="twitter:creator" content="@mattschleg">
  
  <meta property="og:site_name" content="Matthew Schlegel">
  <meta property="og:url" content="https://mkschleg.github.io/braindump/reinforcement_learning/">
  <meta property="og:title" content="Reinforcement Learning | Matthew Schlegel">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-08-30T12:54:57-06:00">
  <meta property="article:modified_time" content="2022-08-30T12:54:57-06:00">
  

  <title>Reinforcement Learning | Matthew Schlegel</title>

  

  
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']], 
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

</head>
<body>

<style type="text/css">
  
 
  
 
</style>

<div class="masthead-hero"></div>


<div id="main" role="main">
  <div class="sidebar sticky" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <div class="author-avatar">
    <a href="/">
      
      <img src="/img/me.jpg" alt="Matthew Schlegel" itemprop="image">
      
    </a>
  </div>
  <div class="author-content">
    <h3 class="author-name" itemprop="name">Matthew Schlegel</h3>
    <p class="author-bio" itemprop="description">Lover of Espresso; PhD student at Amii, RLAI and UofA; Works on how machines perceive their world.</p>
  </div>
  <div class="author-urls-wrapper">
    <ul class="author-urls social-icons" aria-hidden="true">
      <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
        <span itemprop="name">Edmonton, Alberta</span>
      </li>
      
      <li>
        <a itemprop="sameAs" href="//linkedin.com/in/mkschleg" target="_blank" rel="noopener noreferrer">
          <i class="fa fa-linkedin"></i>
          LinkedIn
        </a>
      </li>
      
      <li>
        <a itemprop="sameAs" href="//twitter.com/mattschleg" target="_blank" rel="noopener noreferrer">
          <i class="fa fa-twitter"></i>
          Twitter
        </a>
      </li>
      
      <li>
        <a itemprop="sameAs" href="//github.com/mkschleg" target="_blank" rel="noopener noreferrer">
          <i class="fa fa-github"></i>
          Github
        </a>
      </li>
      
    </ul>
    <ul class="author-urls social-icons" aria-hidden="true" style="margin-top:30px;">
      
      <li>
        <a itemprop="sameAs" href=/tags >
          <i class="fa fa-tag"></i>
          Tags
        </a>
      </li>
      
      <li>
        <a itemprop="sameAs" href=/collections >
          <i class="fa fa-folder"></i>
          Collections
        </a>
      </li>
      
    </ul>
  </div>
</div>

  <article class="page">
		<div class="page_container">
			<section class="page_content">
				<div class="navbar-hero">
  <nav>
    
    
    <a class="hover" href="/">Home</a>
    
    
    <a class="hover" href="/about">About</a>
    
    
    <a class="hover" href="/CV.pdf">CV</a>
    
    
    <a class="hover" href="/publications">Publications</a>
    
    
    <a class="hover" href="/code">Code</a>
    
    
    <a class="hover" href=/braindump> BrainDump </a>
    
  </nav>
</div>

				<article class="post" itemscope itemtype="http://schema.org/Article">
  <div class="post-container">
    <h1 itemprop="name"><a href="https://mkschleg.github.io/braindump/reinforcement_learning/">Reinforcement Learning</a></h1>

    
      

<div class="post-metadata">

  <span class="post-date">
    
    <time datetime="2022-08-30 12:54:57 -0600 MDT" itemprop="datePublished dateModified">
      Aug 30, 2022
    </time>
  </span>

  

</div>

    

    <div class="post-style" itemprop="articleBody">
      
      <p>Some major categories of reinforcement learning:</p>
<h2 id="off-policy">Off-policy</h2>
<h2 id="successor-representations">Successor Representations</h2>
<h3 id="b01b2b"><span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_1">Barreto, Borsa, et al. 2018</a>)</h3>
<h3 id="89e7f4"><span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_2">Barreto, Dabney, et al. 2018</a>)</h3>
<h3 id="c71cbb"><span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_5">Lehnert, Tellex, and Littman 2017</a>)</h3>
<h3 id="3de7db"><span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_3">Borsa et al. 2018</a>)</h3>
<h3 id="dc52fb"><span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_4">Hansen et al. 2020</a>)</h3>
<h3 id="39da36"><span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_6">Sherstan, Machado, and Pilarski 2018</a>)</h3>
<h2 id="reading">Reading</h2>
<h3 id="textbooks">Textbooks:</h3>
<ul>
<li><a href="http://www.incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (<a href="#citeproc_bib_item_8">Sutton and Barto 2018</a>)</li>
</ul>
<p>(<a href="#citeproc_bib_item_7">Sutton 2020</a>)</p>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Barreto, Andre, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. 2018. “Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement.” In <i>International Conference on Machine Learning</i>. PMLR.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>Barreto, André, Will Dabney, Rémi Munos, Jonathan J. Hunt, Tom Schaul, Hado van Hasselt, and David Silver. 2018. “Successor Features for Transfer in Reinforcement Learning.” <i>arXiv:1606.05312 [Cs]</i>. <a href="https://arxiv.org/abs/1606.05312">https://arxiv.org/abs/1606.05312</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>Borsa, Diana, André Barreto, John Quan, Daniel Mankowitz, Rémi Munos, Hado van Hasselt, David Silver, and Tom Schaul. 2018. “Universal Successor Features Approximators.” <i>arXiv:1812.07626 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1812.07626">https://arxiv.org/abs/1812.07626</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>Hansen, Steven, Will Dabney, Andre Barreto, Tom Van de Wiele, David Warde-Farley, and Volodymyr Mnih. 2020. “Fast Task Inference with Variational Intrinsic Successor Features.” <i>arXiv:1906.05030 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1906.05030">https://arxiv.org/abs/1906.05030</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a>Lehnert, Lucas, Stefanie Tellex, and Michael L. Littman. 2017. “Advantages and Limitations of Using Successor Features for Transfer in Reinforcement Learning.” <i>arXiv:1708.00102 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1708.00102">https://arxiv.org/abs/1708.00102</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a>Sherstan, Craig, Marlos C. Machado, and Patrick M. Pilarski. 2018. “Accelerating Learning in Constructive Predictive Frameworks with the Successor Representation.” <i>arXiv:1803.09001 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1803.09001">https://arxiv.org/abs/1803.09001</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a>Sutton, Richard S. 2020. “John McCarthy’s Definition of Intelligence.” <i>Journal of Artificial General Intelligence</i>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a>Sutton, Richard S., and Andrew G. Barto. 2018. <i>Reinforcement Learning: An Introduction</i>. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press.</div>
</div>
    </div>

    
    


<div class="post-tags">
  <h4> Tags: </h4>
  
  <a href="/tags/Reinforcement-Learning">Reinforcement-Learning</a>
  
</div>



    

    
    

    

    

    

    
    


   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   
      
   

   
      
   

   
      
   

   
      
   

   

   

   

   

   

   
      
   

   
      
   

   
      
   

   

   
      
   

   
      
   

   
      
   

   

   

   

   
      
   

   

   

   
      
   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   
      
   

   
      
   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   
      
   

   

   
      
   

   

   

   

   
      
   

   

   

   

   

   

   
      
   

   

   
      
   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   
      
   

   
      
   

   

   
      
   

   

   
      
   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
      <ul>
       
          <li><a href="/braindump/predictive_processing/">Predictive Processing</a></li>
       
          <li><a href="/braindump/jaderberg2017/">jaderberg2017: Reinforcement Learning with Unsupervised Auxiliary Tasks</a></li>
       
          <li><a href="/braindump/deepmind_lab/">DeepMind Lab</a></li>
       
          <li><a href="/braindump/experience_replay/">Experience Replay</a></li>
       
          <li><a href="/braindump/jaderberg2017/">mnih2016asynchronous: Asynchronous Methods for Deep Reinforcement Learning</a></li>
       
          <li><a href="/braindump/white2017/">white2017: Unifying Task Specification in Reinforcement Learning</a></li>
       
          <li><a href="/braindump/white2015/">white2015: Developing a predictive approach to knowledge</a></li>
       
          <li><a href="/braindump/wang2017/">wang2017: Learning to reinforcement learn</a></li>
       
          <li><a href="/braindump/veeriah2019/">veeriah2019: Discovery of Useful Questions as Auxiliary Tasks</a></li>
       
          <li><a href="/braindump/vanhasselt2015/">vanhasselt2015: Learning to Predict Independent of Span</a></li>
       
          <li><a href="/braindump/value_function/">Value Function</a></li>
       
          <li><a href="/braindump/temporal_difference_learning/">Temporal Difference Learning</a></li>
       
          <li><a href="/braindump/sutton2011/">sutton2011: Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction</a></li>
       
          <li><a href="/braindump/subramanian2020/">subramanian2020: Approximate information state for approximate planning and reinforcement learning in partially observed systems</a></li>
       
          <li><a href="/braindump/causality_for_machine_learning/">scholkopf2019: Causality for Machine Learning</a></li>
       
          <li><a href="/braindump/rich_sutton/">Rich Sutton</a></li>
       
          <li><a href="/braindump/reward/">Reward</a></li>
       
          <li><a href="/braindump/reinforcement_learning_an_introduction/">Reinforcement Learning: An Introduction</a></li>
       
          <li><a href="/braindump/model_based_rl/">Model-based RL</a></li>
       
          <li><a href="/braindump/markov_decisions_process/">Markov Decisions Process</a></li>
       
          <li><a href="/braindump/machado2018/">machado2018: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents</a></li>
       
          <li><a href="/braindump/liu2018/">liu2018: Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation</a></li>
       
          <li><a href="/braindump/sutton1988/">sutton1988: Learning to predict by the methods of temporal differences</a></li>
       
          <li><a href="/braindump/kostas2019/">kostas2019: Asynchronous Coagent Networks: Stochastic Networks for Reinforcement Learning without Backpropagation or a Clock</a></li>
       
          <li><a href="/braindump/kearney2019/">kearney2019: Making Meaning: Semiotics Within Predictive Knowledge Architectures</a></li>
       
          <li><a href="/braindump/deep_reinforcement_learning/">Deep Reinforcement Learning</a></li>
       
          <li><a href="/braindump/dayan1993/">dayan1993: Improving Generalization for Temporal Difference Learning: The Successor Representation</a></li>
       
          <li><a href="/braindump/behavior_suite/">Behavior-Suite</a></li>
       
          <li><a href="/braindump/barreto2018a/">barreto2018a: Successor Features for Transfer in Reinforcement Learning</a></li>
       
          <li><a href="/braindump/badia2020/">badia2020: Agent57: Outperforming the Atari Human Benchmark</a></li>
       
          <li><a href="/braindump/auxiliary_tasks/">Auxiliary Tasks</a></li>
       
          <li><a href="/braindump/atari/">Atari</a></li>
       
          <li><a href="/braindump/artificial_intelligence/">Artificial Intelligence</a></li>
       
     </ul>
    </div>
  </div>


    

  </div>
</article>

			</section>
		</div>
	</article>
</div> 


<div class="page_footer">
	<p>Copyright © 2020 Matthew Schlegel. All Rights Reserved. Powered by <a href="http://gohugo.io/">Hugo</a> and <a href="https://github.com/jhu247/minimal-academic">Minimal Academic</a>.</p>
</div>
    
    


  </body>
</html>

