<!DOCTYPE html><html lang="en"><!-- - var pageLinks = ["home.html","education.html","publications.html","projects.html","musings.html"]--><!-- - var pageNames = ["Home","Education","Research","Projects","Musings"]--><!-- - var styleSheets = ["Home" : "temp.css", "Publications" : "temp.css","Projects" : "temp.css","Code" : "temp.css","Musings" : "temp.css"]--><head><title>Matthew Schlegel - Home</title><meta charset="UTF-8"><link rel="stylesheet" href="assets/styles_2/temp.css"><link rel="stylesheet" href="assets/styles_2/header.css"><link rel="stylesheet" href="assets/styles_2/footer.css"><link rel="stylesheet" href="assets/styles_2/home.css"><link rel="stylesheet" href="assets/styles_2/quotes.css"><link rel="stylesheet" href="assets/styles_2/publications.css"></head><body class="Site"><header><h1 class="headTitle">Matthew Schlegel...</h1><div class="underTitle"></div><nav></nav></header><main class="content"><div class="lookAtThese-wrap subContent"><div class="lookAtThese"><a class="lookAtMe" href="https://github.com/mkschleg" style="background-image: url(assets/images/github-icon.png);background-color:rgb(60,60,60);background-size:200px 200px; background-position:center;"><div class="lookAtMeContentWrap"><h class="title">Github</h><p>Visit my Github to get a look at some of my open-source code.</p></div></a><a class="lookAtMe" href="https://www.linkedin.com/in/matthew-schlegel-1a00b739/" style="background-image: url(assets/images/linkedin-icon.jpg);background-color:rgb(19,127,176);background-size:200px 200px; background-position:center;"><div class="lookAtMeContentWrap"><h class="title">Linkedin</h><p>Visit my Linkedin profile for up to date information about my skills and current professional work.</p></div></a><a class="lookAtMe" href="http://proceedings.mlr.press/v70/schlegel17a.html" style="background-image: url(assets/images/teleTiming.jpg);background-color:white;"><div class="lookAtMeContentWrap"><h class="title">Adapting Kernel Representations Online Using Submodular Maximization</h></div></a></div></div><div class="aboutContent subContent"><h2>About Me</h2><p>I am a current PhD student at the University of Alberta in Edmontown, working under <a href=https://webdocs.cs.ualberta.ca/~whitem/>Martha White</a>. Previously, I attended Indiana University - Bloomington and recieved a Bachelor of Science in Physics (2015) and Masters of Science in Computer Science (2017). My work focuses on General Value Function's use in creating predictive representations in reinforcement learning, and in the use of kernel representations in online continuing settings.

I am a recent graduate from Indiana University - Bloomington with a Masters degree in Computer Science, and am continuing my studies towards a PhD at the University of Alberta in Edmonton. I work with <a href=https://webdocs.cs.ualberta.ca/~whitem/>Martha White</a> in Machine Learning and
Reinforcement Learning research, more specifically in creating predictive representations with General Value Functions, and . I was recently a Teaching Assistant for the course <em> CSCI-B659: Topics in Artificial Intelligence - Reinforcement Learning for Artificial Intelligence</em>
with <a href=http://adamwhite.ca>Adam White</a> at Indiana University.


</p></div><div class="aboutContent subContent"><h2>Research Interests</h2><p>My reasearch interests primarly fall within the realms of Reinforcement Learning, but I also dable in time series and visual data. My main goal is to create agents
that build their own representations of environments in a continual learning setting. My current work focuses on two types of representation learning. The first is
using General Value Functions as a means to create predictive knowledge representations on the fly through question generation. The second is exploring the use of kernel
representations for prediction in an online setting and for reinforcememtn learning, particularly creating efficient methods to select prototypes for such representations.</p><p>I spend my time away from my research listening to my favorite music, playing clarinet, enjoying a good book or movie, and tinkering with various tech and circuits. I also
work to keep up with tech news and recent research in physics.</p></div><div class="publications subContent"><h2>Publications</h2><div class="paperWrapper"><h2>Adapting kernel representations online using submodular maximization.</h2><p>Matthew Schlegel, Yangchen Pan, and Martha White. International Conference on Machine Learning (ICML), 2017.</p></div><div class="paperWrapper"><h2>Stable predictive representations with general value functions for continual learning.</h2><p>Matthew Schlegel, Adam White, Martha White. Continual Learning and Deep Networks workshop at the Neural Information Processing System Conference, 2017.</p></div></div><div class="favoriteQuotes subContent"><div class="quoteWrapper"><p class="quote">Nirvana is a state of pure blissful knowledge... It has nothing to do with the individual. The ego or its separation is an illusion. Indeed in a certain sense two &quot;I&quot;'s are identical namely when one disregards all special contents - their Karma. The goal of man is to preserve his Karma and to develop it further... when man dies his Karma lives and creates for itself another carrier.</p><p class="author">Erwin Schr√∂dinger, 1994</p></div></div></main><footer id="footer"><div id="footerContent"><ul><li><Copyright>Matthew Schlegel (c) 2017</Copyright></li><li><bold>|</bold></li><li><a href="siteInfo.html">Website Info</a></li></ul></div></footer></body></html>